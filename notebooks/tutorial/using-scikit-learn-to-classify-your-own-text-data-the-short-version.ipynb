{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn To Classify Your Own Text Data (The Short Version)\n",
    "\n",
    "http://carrefax.com/articles-blog/2018/3/11/using-scikit-learn-to-classify-your-own-text-data-the-short-version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colunas = ['ROTULO_MANUAL', 'EMENTA_NORM']\n",
    "\n",
    "df = pd.read_csv('../../data/ementas_pre-processadas.csv', header=0, sep=',', quotechar='\"', usecols=colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = df['EMENTA_NORM'].values.astype('U')\n",
    "target = df['ROTULO_MANUAL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the training data into tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming the training data...\n",
      "\n",
      "(4870, 16346)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "print ('\\nTransforming the training data...\\n')\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(raw_documents=X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the test data into tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming the test data...\n",
      "\n",
      "(3247, 14269)\n",
      "  (0, 7148)\t0.052414241836095915\n",
      "  (0, 9983)\t0.052414241836095915\n",
      "  (0, 2939)\t0.052414241836095915\n",
      "  (0, 11448)\t0.052414241836095915\n",
      "  (0, 2275)\t0.052414241836095915\n",
      "  (0, 3478)\t0.052414241836095915\n",
      "  (0, 329)\t0.052414241836095915\n",
      "  (0, 5336)\t0.052414241836095915\n",
      "  (0, 9537)\t0.052414241836095915\n",
      "  (0, 5643)\t0.052414241836095915\n",
      "  (0, 9438)\t0.052414241836095915\n",
      "  (0, 14083)\t0.052414241836095915\n",
      "  (0, 10781)\t0.052414241836095915\n",
      "  (0, 9666)\t0.052414241836095915\n",
      "  (0, 6356)\t0.052414241836095915\n",
      "  (0, 7105)\t0.052414241836095915\n",
      "  (0, 6577)\t0.052414241836095915\n",
      "  (0, 12927)\t0.052414241836095915\n",
      "  (0, 7861)\t0.052414241836095915\n",
      "  (0, 11586)\t0.052414241836095915\n",
      "  (0, 13065)\t0.052414241836095915\n",
      "  (0, 6866)\t0.052414241836095915\n",
      "  (0, 7800)\t0.052414241836095915\n",
      "  (0, 13479)\t0.052414241836095915\n",
      "  (0, 8210)\t0.052414241836095915\n",
      "  :\t:\n",
      "  (3245, 10108)\t0.13933280001515577\n",
      "  (3245, 3312)\t0.03980937143290165\n",
      "  (3245, 107)\t0.03980937143290165\n",
      "  (3246, 2018)\t0.21320071635561041\n",
      "  (3246, 7869)\t0.21320071635561041\n",
      "  (3246, 11809)\t0.21320071635561041\n",
      "  (3246, 6223)\t0.21320071635561041\n",
      "  (3246, 11621)\t0.21320071635561041\n",
      "  (3246, 3252)\t0.21320071635561041\n",
      "  (3246, 8061)\t0.21320071635561041\n",
      "  (3246, 11577)\t0.21320071635561041\n",
      "  (3246, 7795)\t0.21320071635561041\n",
      "  (3246, 14066)\t0.21320071635561041\n",
      "  (3246, 13452)\t0.21320071635561041\n",
      "  (3246, 651)\t0.21320071635561041\n",
      "  (3246, 3753)\t0.21320071635561041\n",
      "  (3246, 3502)\t0.21320071635561041\n",
      "  (3246, 8231)\t0.21320071635561041\n",
      "  (3246, 5071)\t0.21320071635561041\n",
      "  (3246, 8210)\t0.21320071635561041\n",
      "  (3246, 1269)\t0.21320071635561041\n",
      "  (3246, 11381)\t0.21320071635561041\n",
      "  (3246, 13203)\t0.21320071635561041\n",
      "  (3246, 557)\t0.21320071635561041\n",
      "  (3246, 10108)\t0.21320071635561041\n",
      "(4870,)\n"
     ]
    }
   ],
   "source": [
    "print ('\\nTransforming the test data...\\n')\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_test_counts = count_vect.fit_transform(raw_documents=X_test)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "print (X_test_tfidf.shape)\n",
    "\n",
    "print (X_test_tfidf)\n",
    "print (y_train.shape)\n",
    "\n",
    "docs_test = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 2.24, NNZs: 2766, Bias: -0.878712, T: 4870, Avg. loss: 0.009225\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.61, NNZs: 3425, Bias: -0.908247, T: 9740, Avg. loss: 0.006958\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.38, NNZs: 3749, Bias: -0.921783, T: 14610, Avg. loss: 0.006760\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.28, NNZs: 4249, Bias: -0.931023, T: 19480, Avg. loss: 0.006630\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.22, NNZs: 4560, Bias: -0.937749, T: 24350, Avg. loss: 0.006597\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8.01, NNZs: 8940, Bias: -0.960089, T: 4870, Avg. loss: 0.165634\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.69, NNZs: 9748, Bias: -0.989170, T: 9740, Avg. loss: 0.141044\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.59, NNZs: 10070, Bias: -1.002012, T: 14610, Avg. loss: 0.136748\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.50, NNZs: 10305, Bias: -1.008369, T: 19480, Avg. loss: 0.134980\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.45, NNZs: 10342, Bias: -1.010807, T: 24350, Avg. loss: 0.134087\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5.49, NNZs: 5776, Bias: -0.919454, T: 4870, Avg. loss: 0.050613\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.18, NNZs: 6739, Bias: -0.951161, T: 9740, Avg. loss: 0.041541\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.02, NNZs: 7205, Bias: -0.957642, T: 14610, Avg. loss: 0.039814\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.99, NNZs: 7408, Bias: -0.966939, T: 19480, Avg. loss: 0.039126\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.94, NNZs: 7529, Bias: -0.969364, T: 24350, Avg. loss: 0.038861\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.63, NNZs: 5273, Bias: -0.874635, T: 4870, Avg. loss: 0.041702\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.21, NNZs: 6375, Bias: -0.908355, T: 9740, Avg. loss: 0.035968\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.07, NNZs: 6952, Bias: -0.921190, T: 14610, Avg. loss: 0.034946\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.02, NNZs: 7400, Bias: -0.931596, T: 19480, Avg. loss: 0.034423\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.99, NNZs: 7579, Bias: -0.938833, T: 24350, Avg. loss: 0.034008\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.15, NNZs: 2772, Bias: -0.875254, T: 4870, Avg. loss: 0.008934\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.51, NNZs: 3425, Bias: -0.908063, T: 9740, Avg. loss: 0.006871\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.25, NNZs: 3883, Bias: -0.919757, T: 14610, Avg. loss: 0.006673\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.15, NNZs: 4268, Bias: -0.931643, T: 19480, Avg. loss: 0.006609\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.07, NNZs: 4727, Bias: -0.937299, T: 24350, Avg. loss: 0.006537\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.68, NNZs: 4686, Bias: -0.906419, T: 4870, Avg. loss: 0.037605\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.05, NNZs: 5660, Bias: -0.937882, T: 9740, Avg. loss: 0.031627\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.83, NNZs: 6232, Bias: -0.948377, T: 14610, Avg. loss: 0.030921\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.75, NNZs: 6764, Bias: -0.957633, T: 19480, Avg. loss: 0.030545\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.69, NNZs: 7156, Bias: -0.961661, T: 24350, Avg. loss: 0.030336\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6.12, NNZs: 9111, Bias: -0.871117, T: 4870, Avg. loss: 0.169770\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.66, NNZs: 10308, Bias: -0.891315, T: 9740, Avg. loss: 0.147987\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.53, NNZs: 10914, Bias: -0.907594, T: 14610, Avg. loss: 0.144486\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.47, NNZs: 11230, Bias: -0.914154, T: 19480, Avg. loss: 0.142773\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.43, NNZs: 11414, Bias: -0.920533, T: 24350, Avg. loss: 0.141945\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7.38, NNZs: 9431, Bias: -0.863996, T: 4870, Avg. loss: 0.172021\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.95, NNZs: 10311, Bias: -0.900589, T: 9740, Avg. loss: 0.144092\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.81, NNZs: 10660, Bias: -0.912034, T: 14610, Avg. loss: 0.140798\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.79, NNZs: 10799, Bias: -0.921084, T: 19480, Avg. loss: 0.139552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.73, NNZs: 10898, Bias: -0.927617, T: 24350, Avg. loss: 0.137750\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.53, NNZs: 5057, Bias: -0.924111, T: 4870, Avg. loss: 0.055846\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.07, NNZs: 6019, Bias: -0.957033, T: 9740, Avg. loss: 0.047494\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.92, NNZs: 6469, Bias: -0.967629, T: 14610, Avg. loss: 0.046143\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.86, NNZs: 6690, Bias: -0.974969, T: 19480, Avg. loss: 0.045716\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.82, NNZs: 6930, Bias: -0.980672, T: 24350, Avg. loss: 0.045397\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.86, NNZs: 3473, Bias: -0.884776, T: 4870, Avg. loss: 0.022993\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.43, NNZs: 4598, Bias: -0.914673, T: 9740, Avg. loss: 0.018328\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.31, NNZs: 4986, Bias: -0.932152, T: 14610, Avg. loss: 0.017324\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.25, NNZs: 5262, Bias: -0.938700, T: 19480, Avg. loss: 0.017433\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.22, NNZs: 5540, Bias: -0.948098, T: 24350, Avg. loss: 0.017007\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9.47, NNZs: 5454, Bias: -1.020013, T: 4870, Avg. loss: 0.107886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.06, NNZs: 6242, Bias: -1.064816, T: 9740, Avg. loss: 0.087323\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.02, NNZs: 6549, Bias: -1.068655, T: 14610, Avg. loss: 0.085044\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.01, NNZs: 6668, Bias: -1.083419, T: 19480, Avg. loss: 0.083297\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 8.96, NNZs: 6757, Bias: -1.093482, T: 24350, Avg. loss: 0.081741\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.99, NNZs: 3674, Bias: -0.890817, T: 4870, Avg. loss: 0.019257\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.56, NNZs: 4321, Bias: -0.921233, T: 9740, Avg. loss: 0.015574\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.46, NNZs: 4787, Bias: -0.940231, T: 14610, Avg. loss: 0.014964\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.38, NNZs: 4921, Bias: -0.944640, T: 19480, Avg. loss: 0.014868\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.35, NNZs: 5066, Bias: -0.951069, T: 24350, Avg. loss: 0.014789\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.33, NNZs: 5533, Bias: -0.845800, T: 4870, Avg. loss: 0.046615\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.91, NNZs: 6368, Bias: -0.877351, T: 9740, Avg. loss: 0.037837\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.80, NNZs: 6972, Bias: -0.895044, T: 14610, Avg. loss: 0.036624\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.75, NNZs: 7397, Bias: -0.902200, T: 19480, Avg. loss: 0.036119\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.72, NNZs: 7748, Bias: -0.909375, T: 24350, Avg. loss: 0.035887\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.25, NNZs: 2526, Bias: -0.863847, T: 4870, Avg. loss: 0.006064\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.69, NNZs: 3298, Bias: -0.897349, T: 9740, Avg. loss: 0.004116\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.47, NNZs: 3671, Bias: -0.910547, T: 14610, Avg. loss: 0.004009\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.37, NNZs: 3959, Bias: -0.920349, T: 19480, Avg. loss: 0.003951\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.30, NNZs: 4329, Bias: -0.925595, T: 24350, Avg. loss: 0.003943\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.57, NNZs: 2971, Bias: -0.848991, T: 4870, Avg. loss: 0.010815\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.01, NNZs: 3861, Bias: -0.885583, T: 9740, Avg. loss: 0.008901\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.79, NNZs: 4258, Bias: -0.899292, T: 14610, Avg. loss: 0.008681\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.71, NNZs: 4684, Bias: -0.909824, T: 19480, Avg. loss: 0.008547\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.65, NNZs: 5155, Bias: -0.915482, T: 24350, Avg. loss: 0.008383\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8.08, NNZs: 5916, Bias: -0.875205, T: 4870, Avg. loss: 0.110660\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.80, NNZs: 6685, Bias: -0.915178, T: 9740, Avg. loss: 0.085409\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.82, NNZs: 6913, Bias: -0.923095, T: 14610, Avg. loss: 0.082601\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.75, NNZs: 6998, Bias: -0.934931, T: 19480, Avg. loss: 0.080636\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.72, NNZs: 7043, Bias: -0.942163, T: 24350, Avg. loss: 0.080088\n",
      "Total training time: 0.01 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 6.85, NNZs: 5991, Bias: -1.006824, T: 4870, Avg. loss: 0.075423\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.43, NNZs: 6884, Bias: -1.055452, T: 9740, Avg. loss: 0.058763\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.21, NNZs: 7317, Bias: -1.072165, T: 14610, Avg. loss: 0.057385\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.24, NNZs: 7484, Bias: -1.083911, T: 19480, Avg. loss: 0.056160\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.18, NNZs: 7553, Bias: -1.095045, T: 24350, Avg. loss: 0.055854\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.33, NNZs: 4238, Bias: -0.848835, T: 4870, Avg. loss: 0.032016\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.97, NNZs: 5286, Bias: -0.887235, T: 9740, Avg. loss: 0.024840\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.87, NNZs: 5713, Bias: -0.901138, T: 14610, Avg. loss: 0.023886\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.82, NNZs: 5957, Bias: -0.906700, T: 19480, Avg. loss: 0.023556\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.79, NNZs: 6071, Bias: -0.912640, T: 24350, Avg. loss: 0.023392\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.57, NNZs: 3689, Bias: -0.857861, T: 4870, Avg. loss: 0.025571\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.09, NNZs: 4506, Bias: -0.888672, T: 9740, Avg. loss: 0.020066\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.93, NNZs: 4940, Bias: -0.899411, T: 14610, Avg. loss: 0.019508\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.89, NNZs: 5352, Bias: -0.913025, T: 19480, Avg. loss: 0.019168\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.84, NNZs: 5474, Bias: -0.916541, T: 24350, Avg. loss: 0.019082\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.98, NNZs: 6797, Bias: -0.833968, T: 4870, Avg. loss: 0.114714\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.47, NNZs: 8063, Bias: -0.858812, T: 9740, Avg. loss: 0.099538\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.33, NNZs: 8654, Bias: -0.874413, T: 14610, Avg. loss: 0.097006\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.24, NNZs: 9000, Bias: -0.879305, T: 19480, Avg. loss: 0.095922\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.20, NNZs: 9118, Bias: -0.885736, T: 24350, Avg. loss: 0.095631\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 1.92, NNZs: 1872, Bias: -0.866572, T: 4870, Avg. loss: 0.001650\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.25, NNZs: 2538, Bias: -0.897182, T: 9740, Avg. loss: 0.000424\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.01, NNZs: 2844, Bias: -0.912647, T: 14610, Avg. loss: 0.000396\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.87, NNZs: 3375, Bias: -0.921802, T: 19480, Avg. loss: 0.000386\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.77, NNZs: 3577, Bias: -0.927538, T: 24350, Avg. loss: 0.000381\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.03, NNZs: 2062, Bias: -0.863025, T: 4870, Avg. loss: 0.003181\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.34, NNZs: 2608, Bias: -0.893836, T: 9740, Avg. loss: 0.001630\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.09, NNZs: 2971, Bias: -0.909789, T: 14610, Avg. loss: 0.001558\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.96, NNZs: 3405, Bias: -0.920703, T: 19480, Avg. loss: 0.001530\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.87, NNZs: 3628, Bias: -0.927239, T: 24350, Avg. loss: 0.001528\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.32, NNZs: 2645, Bias: -0.880013, T: 4870, Avg. loss: 0.009201\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.70, NNZs: 3377, Bias: -0.907630, T: 9740, Avg. loss: 0.007071\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.50, NNZs: 3715, Bias: -0.922448, T: 14610, Avg. loss: 0.006862\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.39, NNZs: 4046, Bias: -0.930492, T: 19480, Avg. loss: 0.006756\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.32, NNZs: 4237, Bias: -0.934640, T: 24350, Avg. loss: 0.006737\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7.19, NNZs: 6471, Bias: -0.925343, T: 4870, Avg. loss: 0.090708\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.64, NNZs: 7256, Bias: -0.950268, T: 9740, Avg. loss: 0.075003\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.47, NNZs: 7644, Bias: -0.957453, T: 14610, Avg. loss: 0.073132\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.46, NNZs: 7882, Bias: -0.970541, T: 19480, Avg. loss: 0.072029\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.41, NNZs: 7995, Bias: -0.974234, T: 24350, Avg. loss: 0.071239\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.44, NNZs: 4797, Bias: -0.894087, T: 4870, Avg. loss: 0.036720\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.21, NNZs: 5537, Bias: -0.935657, T: 9740, Avg. loss: 0.028695\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.14, NNZs: 5905, Bias: -0.946865, T: 14610, Avg. loss: 0.027732\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.11, NNZs: 6153, Bias: -0.959023, T: 19480, Avg. loss: 0.027381\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.10, NNZs: 6326, Bias: -0.965752, T: 24350, Avg. loss: 0.027016\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.40, NNZs: 3280, Bias: -0.880596, T: 4870, Avg. loss: 0.013165\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.85, NNZs: 4152, Bias: -0.920729, T: 9740, Avg. loss: 0.010660\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.62, NNZs: 4581, Bias: -0.931872, T: 14610, Avg. loss: 0.010223\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.53, NNZs: 5013, Bias: -0.941298, T: 19480, Avg. loss: 0.010171\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.48, NNZs: 5378, Bias: -0.948601, T: 24350, Avg. loss: 0.010129\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.11, NNZs: 2659, Bias: -0.867485, T: 4870, Avg. loss: 0.009799\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.48, NNZs: 3351, Bias: -0.899294, T: 9740, Avg. loss: 0.007370\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.24, NNZs: 3748, Bias: -0.912068, T: 14610, Avg. loss: 0.007142\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.13, NNZs: 4247, Bias: -0.922563, T: 19480, Avg. loss: 0.007090\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.03, NNZs: 4478, Bias: -0.925750, T: 24350, Avg. loss: 0.007050\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.08, NNZs: 5719, Bias: -0.903024, T: 4870, Avg. loss: 0.074711\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.44, NNZs: 6836, Bias: -0.933384, T: 9740, Avg. loss: 0.064541\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.28, NNZs: 7353, Bias: -0.948207, T: 14610, Avg. loss: 0.062756\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.19, NNZs: 7697, Bias: -0.951283, T: 19480, Avg. loss: 0.062127\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.15, NNZs: 7885, Bias: -0.955246, T: 24350, Avg. loss: 0.061667\n",
      "Total training time: 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean accuracy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7302125038497074\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labelled performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        AUT       0.83      0.42      0.56        12\n",
      "        BAN       0.56      0.78      0.65       300\n",
      "        BUS       0.72      0.59      0.65       101\n",
      "        CDC       0.76      0.43      0.55        74\n",
      "        CIV       0.00      0.00      0.00        16\n",
      "        COM       0.21      0.13      0.16        46\n",
      "        CON       0.74      0.60      0.66       289\n",
      "        DAN       0.64      0.72      0.68       306\n",
      "        DMI       0.69      0.56      0.62        96\n",
      "        DPV       0.81      0.81      0.81        37\n",
      "        EXP       0.83      0.96      0.89       577\n",
      "        FAL       0.80      0.55      0.65        29\n",
      "        FAM       0.71      0.61      0.66        74\n",
      "        INF       0.64      0.78      0.70         9\n",
      "        MAR       0.75      0.47      0.58        19\n",
      "        OIE       0.76      0.79      0.78       217\n",
      "        OIG       0.87      0.88      0.88       241\n",
      "        POS       0.64      0.79      0.71        43\n",
      "        PRE       0.90      0.75      0.82        63\n",
      "        RAI       0.76      0.43      0.55       184\n",
      "        REG       0.00      0.00      0.00         1\n",
      "      RESCI       1.00      0.29      0.44         7\n",
      "      RESCO       0.50      0.88      0.64         8\n",
      "        SEG       0.73      0.87      0.79       287\n",
      "        SFH       0.70      0.87      0.78        68\n",
      "        SOC       1.00      0.25      0.40         8\n",
      "        SUC       1.00      0.18      0.31        11\n",
      "        TIT       0.78      0.48      0.59       124\n",
      "\n",
      "avg / total       0.73      0.73      0.72      3247\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
