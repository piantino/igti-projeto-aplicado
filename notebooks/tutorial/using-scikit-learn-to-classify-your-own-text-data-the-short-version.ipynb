{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn To Classify Your Own Text Data (The Short Version)\n",
    "\n",
    "http://carrefax.com/articles-blog/2018/3/11/using-scikit-learn-to-classify-your-own-text-data-the-short-version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colunas = ['ROTULO_MANUAL', 'EMENTA_NORM']\n",
    "\n",
    "df = pd.read_csv('../../data/ementas_pre-processadas.csv', header=0, sep=',', quotechar='\"', usecols=colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = df['EMENTA_NORM'].values.astype('U')\n",
    "target = df['ROTULO_MANUAL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the training data into tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming the training data...\n",
      "\n",
      "(4870, 16309)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "print ('\\nTransforming the training data...\\n')\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(raw_documents=X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the test data into tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming the test data...\n",
      "\n",
      "(3247, 14303)\n",
      "  (0, 7184)\t0.06495698024616309\n",
      "  (0, 3507)\t0.06495698024616309\n",
      "  (0, 8886)\t0.06495698024616309\n",
      "  (0, 6751)\t0.06495698024616309\n",
      "  (0, 6377)\t0.06495698024616309\n",
      "  (0, 12751)\t0.06495698024616309\n",
      "  (0, 13388)\t0.06495698024616309\n",
      "  (0, 1172)\t0.06495698024616309\n",
      "  (0, 7142)\t0.06495698024616309\n",
      "  (0, 2259)\t0.06495698024616309\n",
      "  (0, 3483)\t0.06495698024616309\n",
      "  (0, 12622)\t0.06495698024616309\n",
      "  (0, 5324)\t0.06495698024616309\n",
      "  (0, 11613)\t0.06495698024616309\n",
      "  (0, 6197)\t0.06495698024616309\n",
      "  (0, 5619)\t0.12991396049232617\n",
      "  (0, 6374)\t0.06495698024616309\n",
      "  (0, 7051)\t0.06495698024616309\n",
      "  (0, 11628)\t0.06495698024616309\n",
      "  (0, 7186)\t0.06495698024616309\n",
      "  (0, 6615)\t0.06495698024616309\n",
      "  (0, 12962)\t0.06495698024616309\n",
      "  (0, 7880)\t0.06495698024616309\n",
      "  (0, 11626)\t0.06495698024616309\n",
      "  (0, 6905)\t0.06495698024616309\n",
      "  :\t:\n",
      "  (3246, 11657)\t0.10721125348377948\n",
      "  (3246, 224)\t0.10721125348377948\n",
      "  (3246, 10887)\t0.10721125348377948\n",
      "  (3246, 11218)\t0.10721125348377948\n",
      "  (3246, 13385)\t0.10721125348377948\n",
      "  (3246, 7374)\t0.10721125348377948\n",
      "  (3246, 1072)\t0.10721125348377948\n",
      "  (3246, 8869)\t0.10721125348377948\n",
      "  (3246, 7839)\t0.10721125348377948\n",
      "  (3246, 3614)\t0.10721125348377948\n",
      "  (3246, 7821)\t0.10721125348377948\n",
      "  (3246, 562)\t0.10721125348377948\n",
      "  (3246, 3985)\t0.10721125348377948\n",
      "  (3246, 3772)\t0.10721125348377948\n",
      "  (3246, 10716)\t0.10721125348377948\n",
      "  (3246, 9705)\t0.10721125348377948\n",
      "  (3246, 9543)\t0.10721125348377948\n",
      "  (3246, 3257)\t0.10721125348377948\n",
      "  (3246, 3758)\t0.21442250696755896\n",
      "  (3246, 5087)\t0.3216337604513384\n",
      "  (3246, 3507)\t0.21442250696755896\n",
      "  (3246, 11628)\t0.10721125348377948\n",
      "  (3246, 1275)\t0.21442250696755896\n",
      "  (3246, 3749)\t0.21442250696755896\n",
      "  (3246, 12680)\t0.10721125348377948\n",
      "(4870,)\n"
     ]
    }
   ],
   "source": [
    "print ('\\nTransforming the test data...\\n')\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_test_counts = count_vect.fit_transform(raw_documents=X_test)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "print (X_test_tfidf.shape)\n",
    "\n",
    "print (X_test_tfidf)\n",
    "print (y_train.shape)\n",
    "\n",
    "docs_test = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 2.31, NNZs: 2759, Bias: -0.851990, T: 4870, Avg. loss: 0.007635\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.70, NNZs: 3514, Bias: -0.892791, T: 9740, Avg. loss: 0.006059\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.45, NNZs: 3898, Bias: -0.908031, T: 14610, Avg. loss: 0.005939\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.33, NNZs: 4262, Bias: -0.917604, T: 19480, Avg. loss: 0.005864\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.25, NNZs: 4569, Bias: -0.923024, T: 24350, Avg. loss: 0.005817\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7.46, NNZs: 8607, Bias: -0.950215, T: 4870, Avg. loss: 0.170526\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.02, NNZs: 9585, Bias: -0.980967, T: 9740, Avg. loss: 0.143398\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.94, NNZs: 10009, Bias: -0.988459, T: 14610, Avg. loss: 0.140561\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.78, NNZs: 10138, Bias: -0.994628, T: 19480, Avg. loss: 0.138382\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.80, NNZs: 10231, Bias: -0.997292, T: 24350, Avg. loss: 0.137594\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5.35, NNZs: 5536, Bias: -0.896221, T: 4870, Avg. loss: 0.052714\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.09, NNZs: 6634, Bias: -0.930550, T: 9740, Avg. loss: 0.043457\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.99, NNZs: 7245, Bias: -0.947250, T: 14610, Avg. loss: 0.040912\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.95, NNZs: 7465, Bias: -0.957651, T: 19480, Avg. loss: 0.040425\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.93, NNZs: 7647, Bias: -0.965348, T: 24350, Avg. loss: 0.040095\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.86, NNZs: 5427, Bias: -0.870421, T: 4870, Avg. loss: 0.044251\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.37, NNZs: 6554, Bias: -0.904122, T: 9740, Avg. loss: 0.038342\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.22, NNZs: 7159, Bias: -0.915979, T: 14610, Avg. loss: 0.037870\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.15, NNZs: 7614, Bias: -0.923918, T: 19480, Avg. loss: 0.037014\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.11, NNZs: 7903, Bias: -0.928312, T: 24350, Avg. loss: 0.036782\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.30, NNZs: 3033, Bias: -0.852901, T: 4870, Avg. loss: 0.011163\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.74, NNZs: 4069, Bias: -0.896522, T: 9740, Avg. loss: 0.009047\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.48, NNZs: 4557, Bias: -0.910800, T: 14610, Avg. loss: 0.008839\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.35, NNZs: 5058, Bias: -0.920054, T: 19480, Avg. loss: 0.008722\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.29, NNZs: 5379, Bias: -0.927561, T: 24350, Avg. loss: 0.008679\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.65, NNZs: 5046, Bias: -0.900781, T: 4870, Avg. loss: 0.037290\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.01, NNZs: 6035, Bias: -0.934694, T: 9740, Avg. loss: 0.030653\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.79, NNZs: 6784, Bias: -0.946159, T: 14610, Avg. loss: 0.029864\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.68, NNZs: 7321, Bias: -0.953815, T: 19480, Avg. loss: 0.029524\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.64, NNZs: 7699, Bias: -0.960439, T: 24350, Avg. loss: 0.029334\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6.32, NNZs: 9570, Bias: -0.839667, T: 4870, Avg. loss: 0.180607\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.79, NNZs: 10781, Bias: -0.863645, T: 9740, Avg. loss: 0.153916\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.65, NNZs: 11544, Bias: -0.891042, T: 14610, Avg. loss: 0.150482\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.57, NNZs: 11744, Bias: -0.897722, T: 19480, Avg. loss: 0.148753\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.52, NNZs: 11819, Bias: -0.904194, T: 24350, Avg. loss: 0.148147\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7.77, NNZs: 9439, Bias: -0.869949, T: 4870, Avg. loss: 0.164339\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.36, NNZs: 10252, Bias: -0.906722, T: 9740, Avg. loss: 0.139192\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.19, NNZs: 10586, Bias: -0.920367, T: 14610, Avg. loss: 0.136209\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.10, NNZs: 10870, Bias: -0.929184, T: 19480, Avg. loss: 0.134361\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.09, NNZs: 10995, Bias: -0.935787, T: 24350, Avg. loss: 0.133023\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.67, NNZs: 5188, Bias: -0.893181, T: 4870, Avg. loss: 0.056900\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.19, NNZs: 6071, Bias: -0.929283, T: 9740, Avg. loss: 0.048660\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.03, NNZs: 6480, Bias: -0.942603, T: 14610, Avg. loss: 0.047352\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.97, NNZs: 6816, Bias: -0.951826, T: 19480, Avg. loss: 0.046783\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.94, NNZs: 6945, Bias: -0.959195, T: 24350, Avg. loss: 0.046507\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.31, NNZs: 3406, Bias: -0.876934, T: 4870, Avg. loss: 0.022687\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.79, NNZs: 4198, Bias: -0.910416, T: 9740, Avg. loss: 0.018350\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.63, NNZs: 4686, Bias: -0.927896, T: 14610, Avg. loss: 0.017937\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.55, NNZs: 5067, Bias: -0.934434, T: 19480, Avg. loss: 0.017549\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.52, NNZs: 5304, Bias: -0.944394, T: 24350, Avg. loss: 0.017346\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9.85, NNZs: 5657, Bias: -1.034082, T: 4870, Avg. loss: 0.119102\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.47, NNZs: 6325, Bias: -1.069978, T: 9740, Avg. loss: 0.090423\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.27, NNZs: 6601, Bias: -1.087594, T: 14610, Avg. loss: 0.089250\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.22, NNZs: 6792, Bias: -1.097874, T: 19480, Avg. loss: 0.087314\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.23, NNZs: 6854, Bias: -1.102527, T: 24350, Avg. loss: 0.086920\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.46, NNZs: 4069, Bias: -0.858896, T: 4870, Avg. loss: 0.020967\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.02, NNZs: 4845, Bias: -0.899930, T: 9740, Avg. loss: 0.016431\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.86, NNZs: 5197, Bias: -0.911581, T: 14610, Avg. loss: 0.015789\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.79, NNZs: 5636, Bias: -0.920475, T: 19480, Avg. loss: 0.015458\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.75, NNZs: 5822, Bias: -0.928529, T: 24350, Avg. loss: 0.015283\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.94, NNZs: 5369, Bias: -0.819553, T: 4870, Avg. loss: 0.045541\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.50, NNZs: 6531, Bias: -0.863368, T: 9740, Avg. loss: 0.038252\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.33, NNZs: 7021, Bias: -0.875679, T: 14610, Avg. loss: 0.037418\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.27, NNZs: 7403, Bias: -0.886612, T: 19480, Avg. loss: 0.036909\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.23, NNZs: 7679, Bias: -0.892846, T: 24350, Avg. loss: 0.036744\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.30, NNZs: 2689, Bias: -0.851292, T: 4870, Avg. loss: 0.006500\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.77, NNZs: 3419, Bias: -0.889061, T: 9740, Avg. loss: 0.004363\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.56, NNZs: 3756, Bias: -0.903979, T: 14610, Avg. loss: 0.004314\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.45, NNZs: 4128, Bias: -0.912606, T: 19480, Avg. loss: 0.004269\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.39, NNZs: 4441, Bias: -0.919875, T: 24350, Avg. loss: 0.004148\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.53, NNZs: 3392, Bias: -0.845160, T: 4870, Avg. loss: 0.011650\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.91, NNZs: 4049, Bias: -0.880733, T: 9740, Avg. loss: 0.009739\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.67, NNZs: 4645, Bias: -0.893001, T: 14610, Avg. loss: 0.009483\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.57, NNZs: 5001, Bias: -0.903168, T: 19480, Avg. loss: 0.009396\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm: 1.50, NNZs: 5252, Bias: -0.909105, T: 24350, Avg. loss: 0.009344\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7.94, NNZs: 6113, Bias: -0.907777, T: 4870, Avg. loss: 0.102697\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.62, NNZs: 6673, Bias: -0.932918, T: 9740, Avg. loss: 0.085403\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.54, NNZs: 7047, Bias: -0.960784, T: 14610, Avg. loss: 0.081696\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.56, NNZs: 7123, Bias: -0.964785, T: 19480, Avg. loss: 0.080996\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.53, NNZs: 7205, Bias: -0.977663, T: 24350, Avg. loss: 0.079480\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6.62, NNZs: 5759, Bias: -1.026674, T: 4870, Avg. loss: 0.073468\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.17, NNZs: 6871, Bias: -1.067427, T: 9740, Avg. loss: 0.062956\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.10, NNZs: 7134, Bias: -1.083460, T: 14610, Avg. loss: 0.060810\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.04, NNZs: 7404, Bias: -1.096309, T: 19480, Avg. loss: 0.059887\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.98, NNZs: 7537, Bias: -1.103425, T: 24350, Avg. loss: 0.059216\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.89, NNZs: 4350, Bias: -0.841489, T: 4870, Avg. loss: 0.025313\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.53, NNZs: 5137, Bias: -0.872082, T: 9740, Avg. loss: 0.020539\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.42, NNZs: 5737, Bias: -0.884878, T: 14610, Avg. loss: 0.019684\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.37, NNZs: 5970, Bias: -0.893732, T: 19480, Avg. loss: 0.019592\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.34, NNZs: 6133, Bias: -0.899141, T: 24350, Avg. loss: 0.019388\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.96, NNZs: 4074, Bias: -0.836340, T: 4870, Avg. loss: 0.026583\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.69, NNZs: 4895, Bias: -0.873456, T: 9740, Avg. loss: 0.021876\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.60, NNZs: 5516, Bias: -0.888043, T: 14610, Avg. loss: 0.020833\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.56, NNZs: 5851, Bias: -0.896462, T: 19480, Avg. loss: 0.020354\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.53, NNZs: 6078, Bias: -0.901328, T: 24350, Avg. loss: 0.020181\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.82, NNZs: 6751, Bias: -0.809070, T: 4870, Avg. loss: 0.111318\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.36, NNZs: 7908, Bias: -0.856928, T: 9740, Avg. loss: 0.095473\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.20, NNZs: 8524, Bias: -0.869962, T: 14610, Avg. loss: 0.093207\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.12, NNZs: 8898, Bias: -0.875652, T: 19480, Avg. loss: 0.092186\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.08, NNZs: 9075, Bias: -0.885266, T: 24350, Avg. loss: 0.091593\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.05, NNZs: 2277, Bias: -0.842362, T: 4870, Avg. loss: 0.001376\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.43, NNZs: 2865, Bias: -0.884106, T: 9740, Avg. loss: 0.000456\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.11, NNZs: 3137, Bias: -0.897367, T: 14610, Avg. loss: 0.000408\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.95, NNZs: 3598, Bias: -0.907066, T: 19480, Avg. loss: 0.000397\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.84, NNZs: 3796, Bias: -0.913818, T: 24350, Avg. loss: 0.000385\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.08, NNZs: 2409, Bias: -0.842035, T: 4870, Avg. loss: 0.004084\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.50, NNZs: 3008, Bias: -0.883765, T: 9740, Avg. loss: 0.002729\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.24, NNZs: 3595, Bias: -0.898638, T: 14610, Avg. loss: 0.002580\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.12, NNZs: 3875, Bias: -0.909849, T: 19480, Avg. loss: 0.002567\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.03, NNZs: 4051, Bias: -0.916252, T: 24350, Avg. loss: 0.002520\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.25, NNZs: 2659, Bias: -0.860596, T: 4870, Avg. loss: 0.007139\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.64, NNZs: 3379, Bias: -0.896166, T: 9740, Avg. loss: 0.005329\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.39, NNZs: 3823, Bias: -0.908769, T: 14610, Avg. loss: 0.005164\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.27, NNZs: 4179, Bias: -0.917900, T: 19480, Avg. loss: 0.005070\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.20, NNZs: 4341, Bias: -0.923798, T: 24350, Avg. loss: 0.005025\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7.09, NNZs: 6384, Bias: -0.889714, T: 4870, Avg. loss: 0.093154\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.69, NNZs: 7413, Bias: -0.930107, T: 9740, Avg. loss: 0.076914\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.55, NNZs: 7767, Bias: -0.934915, T: 14610, Avg. loss: 0.074970\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.52, NNZs: 8025, Bias: -0.944281, T: 19480, Avg. loss: 0.074142\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.48, NNZs: 8157, Bias: -0.953150, T: 24350, Avg. loss: 0.073480\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.35, NNZs: 4822, Bias: -0.882729, T: 4870, Avg. loss: 0.033558\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.97, NNZs: 5688, Bias: -0.914624, T: 9740, Avg. loss: 0.026907\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.88, NNZs: 6119, Bias: -0.935436, T: 14610, Avg. loss: 0.024817\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.83, NNZs: 6401, Bias: -0.946497, T: 19480, Avg. loss: 0.024530\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.80, NNZs: 6575, Bias: -0.950247, T: 24350, Avg. loss: 0.024368\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.37, NNZs: 3427, Bias: -0.870029, T: 4870, Avg. loss: 0.011084\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.82, NNZs: 4164, Bias: -0.906366, T: 9740, Avg. loss: 0.008934\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.62, NNZs: 4639, Bias: -0.922123, T: 14610, Avg. loss: 0.008667\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.53, NNZs: 5020, Bias: -0.932248, T: 19480, Avg. loss: 0.008572\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.47, NNZs: 5362, Bias: -0.938975, T: 24350, Avg. loss: 0.008486\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.24, NNZs: 3082, Bias: -0.848981, T: 4870, Avg. loss: 0.010946\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.58, NNZs: 3816, Bias: -0.885006, T: 9740, Avg. loss: 0.008653\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.34, NNZs: 4296, Bias: -0.901738, T: 14610, Avg. loss: 0.008412\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.20, NNZs: 4633, Bias: -0.910747, T: 19480, Avg. loss: 0.008278\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.11, NNZs: 4852, Bias: -0.916012, T: 24350, Avg. loss: 0.008258\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.18, NNZs: 5709, Bias: -0.898463, T: 4870, Avg. loss: 0.074653\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.67, NNZs: 6930, Bias: -0.939403, T: 9740, Avg. loss: 0.064612\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.47, NNZs: 7419, Bias: -0.945456, T: 14610, Avg. loss: 0.062737\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.40, NNZs: 7749, Bias: -0.952466, T: 19480, Avg. loss: 0.061898\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.36, NNZs: 7965, Bias: -0.958513, T: 24350, Avg. loss: 0.061558\n",
      "Total training time: 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    0.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip...ty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean accuracy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7222051124114567\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labelled performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        AUT       1.00      0.36      0.53        14\n",
      "        BAN       0.69      0.79      0.73       330\n",
      "        BUS       0.86      0.57      0.69       100\n",
      "        CDC       0.81      0.39      0.53        66\n",
      "        CIV       0.67      0.20      0.31        10\n",
      "        COM       0.55      0.12      0.20        49\n",
      "        CON       0.64      0.59      0.61       273\n",
      "        DAN       0.59      0.71      0.64       305\n",
      "        DMI       0.70      0.54      0.61        92\n",
      "        DPV       0.95      0.85      0.90        46\n",
      "        EXP       0.82      0.97      0.89       553\n",
      "        FAL       0.62      0.57      0.59        23\n",
      "        FAM       0.59      0.68      0.63        81\n",
      "        INF       0.86      0.75      0.80         8\n",
      "        MAR       0.67      0.56      0.61        18\n",
      "        OIE       0.76      0.78      0.77       228\n",
      "        OIG       0.76      0.89      0.82       242\n",
      "        POS       0.75      0.67      0.71        60\n",
      "        PRE       0.82      0.72      0.77        50\n",
      "        RAI       0.57      0.42      0.48       196\n",
      "        REG       0.00      0.00      0.00         1\n",
      "      RESCI       0.00      0.00      0.00         4\n",
      "      RESCO       0.71      0.38      0.50        13\n",
      "        SEG       0.74      0.84      0.79       264\n",
      "        SFH       0.83      0.78      0.80        80\n",
      "        SOC       1.00      0.17      0.29        12\n",
      "        SUC       0.50      0.12      0.20         8\n",
      "        TIT       0.66      0.50      0.57       121\n",
      "\n",
      "avg / total       0.72      0.72      0.71      3247\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
