{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Scikit-Learn To Classify Your Own Text Data (The Short Version)\n",
    "\n",
    "http://carrefax.com/articles-blog/2018/3/11/using-scikit-learn-to-classify-your-own-text-data-the-short-version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "colunas = ['ROTULO_MANUAL', 'EMENTA_NORM']\n",
    "\n",
    "df = pd.read_csv('../../data/ementas_pre-processadas.csv', header=0, sep=',', quotechar='\"', usecols=colunas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = df['EMENTA_NORM'].values.astype('U')\n",
    "target = df['ROTULO_MANUAL']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the training data into tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming the training data...\n",
      "\n",
      "(4870, 16372)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "print ('\\nTransforming the training data...\\n')\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(raw_documents=X_train)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "print (X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the test data into tfidf vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforming the test data...\n",
      "\n",
      "(3247, 14223)\n",
      "  (0, 13417)\t0.0663723311599972\n",
      "  (0, 13329)\t0.0663723311599972\n",
      "  (0, 12329)\t0.0663723311599972\n",
      "  (0, 185)\t0.0663723311599972\n",
      "  (0, 11142)\t0.0663723311599972\n",
      "  (0, 11578)\t0.0663723311599972\n",
      "  (0, 8233)\t0.0663723311599972\n",
      "  (0, 10898)\t0.0663723311599972\n",
      "  (0, 10379)\t0.0663723311599972\n",
      "  (0, 11366)\t0.0663723311599972\n",
      "  (0, 1860)\t0.0663723311599972\n",
      "  (0, 2183)\t0.0663723311599972\n",
      "  (0, 1719)\t0.0663723311599972\n",
      "  (0, 6317)\t0.0663723311599972\n",
      "  (0, 7313)\t0.0663723311599972\n",
      "  (0, 12615)\t0.0663723311599972\n",
      "  (0, 6280)\t0.0663723311599972\n",
      "  (0, 7766)\t0.0663723311599972\n",
      "  (0, 1514)\t0.0663723311599972\n",
      "  (0, 9946)\t0.0663723311599972\n",
      "  (0, 6281)\t0.0663723311599972\n",
      "  (0, 9225)\t0.0663723311599972\n",
      "  (0, 6306)\t0.0663723311599972\n",
      "  (0, 2579)\t0.0663723311599972\n",
      "  (0, 6181)\t0.0663723311599972\n",
      "  :\t:\n",
      "  (3246, 11606)\t0.16666666666666666\n",
      "  (3246, 8840)\t0.16666666666666666\n",
      "  (3246, 9502)\t0.16666666666666666\n",
      "  (3246, 3240)\t0.16666666666666666\n",
      "  (3246, 11064)\t0.16666666666666666\n",
      "  (3246, 3749)\t0.16666666666666666\n",
      "  (3246, 11797)\t0.16666666666666666\n",
      "  (3246, 9650)\t0.16666666666666666\n",
      "  (3246, 3752)\t0.16666666666666666\n",
      "  (3246, 5056)\t0.3333333333333333\n",
      "  (3246, 14131)\t0.16666666666666666\n",
      "  (3246, 9560)\t0.16666666666666666\n",
      "  (3246, 8831)\t0.16666666666666666\n",
      "  (3246, 9940)\t0.16666666666666666\n",
      "  (3246, 9118)\t0.16666666666666666\n",
      "  (3246, 3657)\t0.16666666666666666\n",
      "  (3246, 7412)\t0.16666666666666666\n",
      "  (3246, 2393)\t0.16666666666666666\n",
      "  (3246, 2324)\t0.16666666666666666\n",
      "  (3246, 1291)\t0.16666666666666666\n",
      "  (3246, 5795)\t0.16666666666666666\n",
      "  (3246, 99)\t0.16666666666666666\n",
      "  (3246, 1004)\t0.16666666666666666\n",
      "  (3246, 2325)\t0.16666666666666666\n",
      "  (3246, 10899)\t0.16666666666666666\n",
      "(4870,)\n"
     ]
    }
   ],
   "source": [
    "print ('\\nTransforming the test data...\\n')\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "X_test_counts = count_vect.fit_transform(raw_documents=X_test)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer(use_idf=False)\n",
    "X_test_tfidf = tfidf_transformer.fit_transform(X_test_counts)\n",
    "print (X_test_tfidf.shape)\n",
    "\n",
    "print (X_test_tfidf)\n",
    "print (y_train.shape)\n",
    "\n",
    "docs_test = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the classifier pipeline using a SGDClassifier algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying the classifier...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "print ('\\nApplying the classifier...\\n')\n",
    "text_clf = Pipeline([('vect', CountVectorizer(stop_words='english')),\n",
    "                     ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                      alpha=1e-3, random_state=42, verbose=1)),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the model to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/andre/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:128: FutureWarning: max_iter and tol parameters have been added in <class 'sklearn.linear_model.stochastic_gradient.SGDClassifier'> in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  \"and default tol will be 1e-3.\" % type(self), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 2.32, NNZs: 3012, Bias: -0.851659, T: 4870, Avg. loss: 0.008491\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.78, NNZs: 3815, Bias: -0.890915, T: 9740, Avg. loss: 0.006714\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.55, NNZs: 4143, Bias: -0.906050, T: 14610, Avg. loss: 0.006425\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.43, NNZs: 4520, Bias: -0.914702, T: 19480, Avg. loss: 0.006361\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.37, NNZs: 4805, Bias: -0.921856, T: 24350, Avg. loss: 0.006334\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7.84, NNZs: 8580, Bias: -0.935586, T: 4870, Avg. loss: 0.162653\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.35, NNZs: 9608, Bias: -0.967003, T: 9740, Avg. loss: 0.137458\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.00, NNZs: 9913, Bias: -0.980665, T: 14610, Avg. loss: 0.133056\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.11, NNZs: 10218, Bias: -0.988118, T: 19480, Avg. loss: 0.132199\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.06, NNZs: 10237, Bias: -0.994443, T: 24350, Avg. loss: 0.131081\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5.20, NNZs: 5707, Bias: -0.899223, T: 4870, Avg. loss: 0.051734\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.75, NNZs: 6666, Bias: -0.927490, T: 9740, Avg. loss: 0.040755\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.69, NNZs: 7098, Bias: -0.946828, T: 14610, Avg. loss: 0.040213\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.64, NNZs: 7432, Bias: -0.955912, T: 19480, Avg. loss: 0.039330\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.61, NNZs: 7614, Bias: -0.960299, T: 24350, Avg. loss: 0.038872\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.46, NNZs: 5195, Bias: -0.867505, T: 4870, Avg. loss: 0.043022\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.90, NNZs: 6277, Bias: -0.899748, T: 9740, Avg. loss: 0.036239\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.74, NNZs: 6925, Bias: -0.917823, T: 14610, Avg. loss: 0.035144\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.67, NNZs: 7351, Bias: -0.926633, T: 19480, Avg. loss: 0.034768\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.62, NNZs: 7749, Bias: -0.933338, T: 24350, Avg. loss: 0.034595\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.24, NNZs: 2964, Bias: -0.860673, T: 4870, Avg. loss: 0.008633\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.57, NNZs: 3678, Bias: -0.894656, T: 9740, Avg. loss: 0.006893\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.27, NNZs: 4033, Bias: -0.906379, T: 14610, Avg. loss: 0.006780\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.17, NNZs: 4499, Bias: -0.919075, T: 19480, Avg. loss: 0.006656\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.08, NNZs: 4715, Bias: -0.923939, T: 24350, Avg. loss: 0.006596\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.79, NNZs: 4637, Bias: -0.866865, T: 4870, Avg. loss: 0.033018\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.15, NNZs: 5640, Bias: -0.909189, T: 9740, Avg. loss: 0.028826\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.90, NNZs: 6218, Bias: -0.924015, T: 14610, Avg. loss: 0.027972\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.79, NNZs: 6914, Bias: -0.932911, T: 19480, Avg. loss: 0.027711\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.72, NNZs: 7212, Bias: -0.939120, T: 24350, Avg. loss: 0.027601\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6.39, NNZs: 9653, Bias: -0.887070, T: 4870, Avg. loss: 0.176851\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 5.89, NNZs: 10905, Bias: -0.912132, T: 9740, Avg. loss: 0.150643\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.75, NNZs: 11308, Bias: -0.924677, T: 14610, Avg. loss: 0.147761\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.67, NNZs: 11478, Bias: -0.928948, T: 19480, Avg. loss: 0.145667\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.65, NNZs: 11656, Bias: -0.937985, T: 24350, Avg. loss: 0.144936\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 7.87, NNZs: 9837, Bias: -0.857274, T: 4870, Avg. loss: 0.178141\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.36, NNZs: 10718, Bias: -0.881500, T: 9740, Avg. loss: 0.149356\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.25, NNZs: 11080, Bias: -0.897085, T: 14610, Avg. loss: 0.145159\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.23, NNZs: 11254, Bias: -0.909047, T: 19480, Avg. loss: 0.142848\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.20, NNZs: 11318, Bias: -0.914475, T: 24350, Avg. loss: 0.141995\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.90, NNZs: 5433, Bias: -0.902900, T: 4870, Avg. loss: 0.056592\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.39, NNZs: 6150, Bias: -0.936326, T: 9740, Avg. loss: 0.048510\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.23, NNZs: 6606, Bias: -0.950949, T: 14610, Avg. loss: 0.047552\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.16, NNZs: 6947, Bias: -0.961636, T: 19480, Avg. loss: 0.047066\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.11, NNZs: 7127, Bias: -0.966886, T: 24350, Avg. loss: 0.046770\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 3.43, NNZs: 3579, Bias: -0.876749, T: 4870, Avg. loss: 0.022408\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.01, NNZs: 4277, Bias: -0.913434, T: 9740, Avg. loss: 0.018190\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.87, NNZs: 4771, Bias: -0.927115, T: 14610, Avg. loss: 0.017671\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.82, NNZs: 5037, Bias: -0.938744, T: 19480, Avg. loss: 0.017198\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.78, NNZs: 5296, Bias: -0.945324, T: 24350, Avg. loss: 0.017158\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 9.76, NNZs: 5520, Bias: -1.005598, T: 4870, Avg. loss: 0.109422\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 9.31, NNZs: 6102, Bias: -1.042540, T: 9740, Avg. loss: 0.084601\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 9.19, NNZs: 6433, Bias: -1.063371, T: 14610, Avg. loss: 0.082017\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 9.17, NNZs: 6609, Bias: -1.073220, T: 19480, Avg. loss: 0.080473\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 9.19, NNZs: 6671, Bias: -1.081321, T: 24350, Avg. loss: 0.079648\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.89, NNZs: 3758, Bias: -0.842405, T: 4870, Avg. loss: 0.019169\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 2.48, NNZs: 4442, Bias: -0.893083, T: 9740, Avg. loss: 0.015928\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 2.29, NNZs: 4854, Bias: -0.906134, T: 14610, Avg. loss: 0.015772\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 2.19, NNZs: 5227, Bias: -0.913837, T: 19480, Avg. loss: 0.015586\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 2.15, NNZs: 5444, Bias: -0.922541, T: 24350, Avg. loss: 0.015417\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.09, NNZs: 5331, Bias: -0.829998, T: 4870, Avg. loss: 0.048107\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.61, NNZs: 6503, Bias: -0.865647, T: 9740, Avg. loss: 0.041524\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.44, NNZs: 7133, Bias: -0.877954, T: 14610, Avg. loss: 0.040159\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.38, NNZs: 7469, Bias: -0.889562, T: 19480, Avg. loss: 0.039622\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.34, NNZs: 7637, Bias: -0.894605, T: 24350, Avg. loss: 0.039258\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.50, NNZs: 2766, Bias: -0.851538, T: 4870, Avg. loss: 0.006232\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.91, NNZs: 3397, Bias: -0.887670, T: 9740, Avg. loss: 0.004376\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.69, NNZs: 3900, Bias: -0.901412, T: 14610, Avg. loss: 0.004454\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.58, NNZs: 4115, Bias: -0.910142, T: 19480, Avg. loss: 0.004275\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.52, NNZs: 4506, Bias: -0.917246, T: 24350, Avg. loss: 0.004260\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.43, NNZs: 2875, Bias: -0.856888, T: 4870, Avg. loss: 0.010353\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.73, NNZs: 3807, Bias: -0.886598, T: 9740, Avg. loss: 0.008486\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.50, NNZs: 4209, Bias: -0.902333, T: 14610, Avg. loss: 0.008349\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.39, NNZs: 4518, Bias: -0.912642, T: 19480, Avg. loss: 0.008216\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.32, NNZs: 4722, Bias: -0.919219, T: 24350, Avg. loss: 0.008186\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 8.10, NNZs: 6300, Bias: -0.868539, T: 4870, Avg. loss: 0.101513\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 7.87, NNZs: 6885, Bias: -0.909866, T: 9740, Avg. loss: 0.082399\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 7.75, NNZs: 7078, Bias: -0.925029, T: 14610, Avg. loss: 0.079057\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 7.77, NNZs: 7156, Bias: -0.938881, T: 19480, Avg. loss: 0.078490\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 7.76, NNZs: 7210, Bias: -0.940270, T: 24350, Avg. loss: 0.077395\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 6.36, NNZs: 5847, Bias: -1.024653, T: 4870, Avg. loss: 0.066019\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.05, NNZs: 6912, Bias: -1.058653, T: 9740, Avg. loss: 0.052130\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 5.86, NNZs: 7303, Bias: -1.076553, T: 14610, Avg. loss: 0.051104\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 5.81, NNZs: 7577, Bias: -1.089560, T: 19480, Avg. loss: 0.050497\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 5.80, NNZs: 7646, Bias: -1.099688, T: 24350, Avg. loss: 0.049991\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.00, NNZs: 4196, Bias: -0.839373, T: 4870, Avg. loss: 0.032725\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.50, NNZs: 5305, Bias: -0.867067, T: 9740, Avg. loss: 0.026260\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.36, NNZs: 5746, Bias: -0.882216, T: 14610, Avg. loss: 0.025256\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.29, NNZs: 6161, Bias: -0.889578, T: 19480, Avg. loss: 0.024908\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.25, NNZs: 6324, Bias: -0.896853, T: 24350, Avg. loss: 0.024633\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.06, NNZs: 4134, Bias: -0.846381, T: 4870, Avg. loss: 0.026510\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.67, NNZs: 5102, Bias: -0.872167, T: 9740, Avg. loss: 0.021425\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.55, NNZs: 5569, Bias: -0.884836, T: 14610, Avg. loss: 0.020451\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.50, NNZs: 5951, Bias: -0.895367, T: 19480, Avg. loss: 0.020012\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.47, NNZs: 6128, Bias: -0.900806, T: 24350, Avg. loss: 0.019777\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 5.13, NNZs: 6751, Bias: -0.825398, T: 4870, Avg. loss: 0.112587\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.53, NNZs: 7893, Bias: -0.846202, T: 9740, Avg. loss: 0.097516\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.37, NNZs: 8444, Bias: -0.861330, T: 14610, Avg. loss: 0.095278\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.30, NNZs: 8755, Bias: -0.870028, T: 19480, Avg. loss: 0.093890\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.25, NNZs: 9062, Bias: -0.875444, T: 24350, Avg. loss: 0.093423\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.06, NNZs: 2328, Bias: -0.846092, T: 4870, Avg. loss: 0.001803\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.41, NNZs: 2969, Bias: -0.884326, T: 9740, Avg. loss: 0.000824\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.11, NNZs: 3391, Bias: -0.897832, T: 14610, Avg. loss: 0.000779\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 0.96, NNZs: 3658, Bias: -0.908421, T: 19480, Avg. loss: 0.000766\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.87, NNZs: 3947, Bias: -0.915540, T: 24350, Avg. loss: 0.000764\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.17, NNZs: 2190, Bias: -0.852532, T: 4870, Avg. loss: 0.003450\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.49, NNZs: 2970, Bias: -0.887492, T: 9740, Avg. loss: 0.002316\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.24, NNZs: 3586, Bias: -0.903977, T: 14610, Avg. loss: 0.002262\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.09, NNZs: 3874, Bias: -0.913583, T: 19480, Avg. loss: 0.002212\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 0.99, NNZs: 4104, Bias: -0.919335, T: 24350, Avg. loss: 0.002198\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.31, NNZs: 2724, Bias: -0.858270, T: 4870, Avg. loss: 0.008151\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.69, NNZs: 3308, Bias: -0.894194, T: 9740, Avg. loss: 0.006108\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.45, NNZs: 3776, Bias: -0.909545, T: 14610, Avg. loss: 0.005827\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.34, NNZs: 4131, Bias: -0.919906, T: 19480, Avg. loss: 0.005817\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.25, NNZs: 4372, Bias: -0.924301, T: 24350, Avg. loss: 0.005790\n",
      "Total training time: 0.02 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 7.37, NNZs: 6499, Bias: -0.870644, T: 4870, Avg. loss: 0.094260\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 6.77, NNZs: 7203, Bias: -0.902528, T: 9740, Avg. loss: 0.075937\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 6.66, NNZs: 7610, Bias: -0.923421, T: 14610, Avg. loss: 0.073900\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 6.55, NNZs: 7902, Bias: -0.933628, T: 19480, Avg. loss: 0.073684\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 6.49, NNZs: 7981, Bias: -0.938484, T: 24350, Avg. loss: 0.073147\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.56, NNZs: 4696, Bias: -0.890474, T: 4870, Avg. loss: 0.032700\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 4.21, NNZs: 5540, Bias: -0.928482, T: 9740, Avg. loss: 0.025219\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 4.12, NNZs: 5952, Bias: -0.943872, T: 14610, Avg. loss: 0.024633\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 4.05, NNZs: 6198, Bias: -0.956616, T: 19480, Avg. loss: 0.023869\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 4.03, NNZs: 6347, Bias: -0.962311, T: 24350, Avg. loss: 0.023657\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.44, NNZs: 3220, Bias: -0.865824, T: 4870, Avg. loss: 0.009483\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.85, NNZs: 4014, Bias: -0.901379, T: 9740, Avg. loss: 0.007667\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.65, NNZs: 4482, Bias: -0.916996, T: 14610, Avg. loss: 0.007387\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.53, NNZs: 4798, Bias: -0.925118, T: 19480, Avg. loss: 0.007316\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.48, NNZs: 5148, Bias: -0.931947, T: 24350, Avg. loss: 0.007247\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 2.25, NNZs: 2964, Bias: -0.847406, T: 4870, Avg. loss: 0.008399\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 1.54, NNZs: 3501, Bias: -0.883018, T: 9740, Avg. loss: 0.006694\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 1.28, NNZs: 3943, Bias: -0.899601, T: 14610, Avg. loss: 0.006415\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 1.12, NNZs: 4302, Bias: -0.907730, T: 19480, Avg. loss: 0.006371\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 1.04, NNZs: 4561, Bias: -0.914635, T: 24350, Avg. loss: 0.006318\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 1\n",
      "Norm: 4.32, NNZs: 6131, Bias: -0.903266, T: 4870, Avg. loss: 0.086051\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 3.78, NNZs: 7176, Bias: -0.934722, T: 9740, Avg. loss: 0.074496\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 3.61, NNZs: 7820, Bias: -0.945777, T: 14610, Avg. loss: 0.072906\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 3.54, NNZs: 8283, Bias: -0.953543, T: 19480, Avg. loss: 0.072252\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 3.49, NNZs: 8505, Bias: -0.956204, T: 24350, Avg. loss: 0.071560\n",
      "Total training time: 0.01 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  28 out of  28 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vect', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        ...ty='l2', power_t=0.5, random_state=42, shuffle=True,\n",
       "       tol=None, verbose=1, warm_start=False))])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the test data into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = text_clf.predict(docs_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate mean accuracy of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.72405297197413\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print (np.mean(predicted == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate labelled performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        AUT       0.80      0.33      0.47        12\n",
      "        BAN       0.62      0.76      0.68       322\n",
      "        BUS       0.76      0.65      0.70       110\n",
      "        CDC       0.82      0.42      0.56        78\n",
      "        CIV       0.00      0.00      0.00        16\n",
      "        COM       0.60      0.06      0.10        53\n",
      "        CON       0.65      0.58      0.62       277\n",
      "        DAN       0.62      0.74      0.68       283\n",
      "        DMI       0.57      0.56      0.56        89\n",
      "        DPV       0.87      0.77      0.81        43\n",
      "        EXP       0.81      0.95      0.87       560\n",
      "        FAL       0.86      0.63      0.73        30\n",
      "        FAM       0.69      0.62      0.65        73\n",
      "        INF       0.75      0.86      0.80         7\n",
      "        MAR       0.86      0.27      0.41        22\n",
      "        OIE       0.71      0.78      0.74       226\n",
      "        OIG       0.89      0.84      0.87       286\n",
      "        POS       0.72      0.84      0.77        49\n",
      "        PRE       0.80      0.75      0.77        52\n",
      "        RAI       0.71      0.45      0.55       188\n",
      "      RESCI       0.67      0.40      0.50         5\n",
      "      RESCO       0.64      0.64      0.64        11\n",
      "        SEG       0.71      0.87      0.78       256\n",
      "        SFH       0.82      0.76      0.79        76\n",
      "        SOC       0.67      0.27      0.38        15\n",
      "        SUC       0.00      0.00      0.00        13\n",
      "        TIT       0.67      0.59      0.63        95\n",
      "\n",
      "avg / total       0.72      0.72      0.71      3247\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.classification_report(y_test, predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
