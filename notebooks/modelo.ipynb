{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificação binária por termos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/piantino/miniconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.25.3) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "from comet_ml import Experiment\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "colunas = ['ROTULO_MANUAL', 'EMENTA_NORM']\n",
    "\n",
    "df = pd.read_csv('../data/ementas_pre-processadas.csv', header=0, sep=',', quotechar='\"', usecols=colunas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rotulos = ['EXP', 'BAN', 'OIG', 'DAN', 'SEG', 'CON', 'OIE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separarAmostras(rotulo, dataframe):\n",
    "   \n",
    "    df = dataframe.copy()\n",
    "    df.loc[df.ROTULO_MANUAL != rotulo, 'ROTULO_MANUAL'] = 'NONE'\n",
    "\n",
    "    quantidade = df['ROTULO_MANUAL'].value_counts()[rotulo]\n",
    "    print(\"Quantidade de amostras do rótulos: {}\".format(quantidade))\n",
    "    \n",
    "    x = df['EMENTA_NORM'].values.astype('U')\n",
    "    y = df['ROTULO_MANUAL'].values\n",
    "    \n",
    "    return train_test_split(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def criarModeloBinario(y_train, y_test):\n",
    "\n",
    "    vect = CountVectorizer()\n",
    "    tfidf = TfidfTransformer()\n",
    "    clf = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "    pipe = Pipeline([\n",
    "        ('vect', vect),\n",
    "        ('tfidf', tfidf),\n",
    "        ('dense', FunctionTransformer(lambda x: x.todense(), accept_sparse=True, validate=True)),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    return pipe.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avaliarModelo(clf, x_test):\n",
    "    y_pred = clf.predict(x_test)\n",
    "    \n",
    "    print(metrics.classification_report(y_test, y_pred))\n",
    "    \n",
    "    return metrics.classification_report(y_test, y_pred, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def converterMetricas(rotulo, result):\n",
    "    metrics_dict = {}\n",
    "    \n",
    "    for metric in result[rotulo].keys():\n",
    "        metrics_dict[rotulo + '-' + metric] = str(result[rotulo][metric])\n",
    "            \n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iniciando experimento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: Experiment is live on comet.ml https://www.comet.ml/piantino/igti-projeto-aplicado/d25158e385af4a1384c025d9a02aea34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(project_name=\"igti-projeto-aplicado\", workspace=\"piantino\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"EXP\" Gerando modelo\n",
      "Quantidade de amostras do rótulos: 1784\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         EXP       0.97      0.88      0.93       477\n",
      "        NONE       0.97      0.99      0.98      2002\n",
      "\n",
      "    accuracy                           0.97      2479\n",
      "   macro avg       0.97      0.94      0.95      2479\n",
      "weighted avg       0.97      0.97      0.97      2479\n",
      "\n",
      "\"BAN\" Gerando modelo\n",
      "Quantidade de amostras do rótulos: 971\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         BAN       0.92      0.38      0.54       255\n",
      "        NONE       0.93      1.00      0.96      2224\n",
      "\n",
      "    accuracy                           0.93      2479\n",
      "   macro avg       0.92      0.69      0.75      2479\n",
      "weighted avg       0.93      0.93      0.92      2479\n",
      "\n",
      "\"OIG\" Gerando modelo\n",
      "Quantidade de amostras do rótulos: 752\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NONE       0.98      1.00      0.99      2304\n",
      "         OIG       0.94      0.78      0.85       175\n",
      "\n",
      "    accuracy                           0.98      2479\n",
      "   macro avg       0.96      0.89      0.92      2479\n",
      "weighted avg       0.98      0.98      0.98      2479\n",
      "\n",
      "\"DAN\" Gerando modelo\n",
      "Quantidade de amostras do rótulos: 888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         DAN       0.84      0.47      0.60       222\n",
      "        NONE       0.95      0.99      0.97      2257\n",
      "\n",
      "    accuracy                           0.94      2479\n",
      "   macro avg       0.89      0.73      0.79      2479\n",
      "weighted avg       0.94      0.94      0.94      2479\n",
      "\n",
      "\"SEG\" Gerando modelo\n",
      "Quantidade de amostras do rótulos: 761\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NONE       0.97      1.00      0.98      2299\n",
      "         SEG       0.96      0.59      0.74       180\n",
      "\n",
      "    accuracy                           0.97      2479\n",
      "   macro avg       0.97      0.80      0.86      2479\n",
      "weighted avg       0.97      0.97      0.97      2479\n",
      "\n",
      "\"CON\" Gerando modelo\n",
      "Quantidade de amostras do rótulos: 832\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CON       0.87      0.14      0.24       233\n",
      "        NONE       0.92      1.00      0.96      2246\n",
      "\n",
      "    accuracy                           0.92      2479\n",
      "   macro avg       0.89      0.57      0.60      2479\n",
      "weighted avg       0.91      0.92      0.89      2479\n",
      "\n",
      "\"OIE\" Gerando modelo\n",
      "Quantidade de amostras do rótulos: 735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        NONE       0.97      1.00      0.98      2293\n",
      "         OIE       0.93      0.61      0.74       186\n",
      "\n",
      "    accuracy                           0.97      2479\n",
      "   macro avg       0.95      0.80      0.86      2479\n",
      "weighted avg       0.97      0.97      0.96      2479\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = 0\n",
    "\n",
    "for rotulo in rotulos:\n",
    "    \n",
    "    print('\"{}\" Gerando modelo'.format(rotulo))\n",
    "    \n",
    "    x_train, x_test, y_train, y_test = separarAmostras(rotulo, df)\n",
    "    \n",
    "    clf = criarModeloBinario(y_train, y_test)\n",
    "    \n",
    "    result = avaliarModelo(clf, x_test)\n",
    "    \n",
    "    accuracy = result['accuracy'] + accuracy\n",
    "    \n",
    "    experiment.log_metrics(converterMetricas(rotulo, result))\n",
    "\n",
    "experiment.log_metric('accuracy', accuracy / len(rotulos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary:\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     url: https://www.comet.ml/piantino/igti-projeto-aplicado/d25158e385af4a1384c025d9a02aea34\n",
      "COMET INFO:   Metrics:\n",
      "COMET INFO:      BAN-f1-score: 0.5373961218836566\n",
      "COMET INFO:     BAN-precision: 0.9150943396226415\n",
      "COMET INFO:        BAN-recall: 0.3803921568627451\n",
      "COMET INFO:       BAN-support: 255\n",
      "COMET INFO:      CON-f1-score: 0.24354243542435428\n",
      "COMET INFO:     CON-precision: 0.868421052631579\n",
      "COMET INFO:        CON-recall: 0.14163090128755365\n",
      "COMET INFO:       CON-support: 233\n",
      "COMET INFO:      DAN-f1-score: 0.6011560693641619\n",
      "COMET INFO:     DAN-precision: 0.8387096774193549\n",
      "COMET INFO:        DAN-recall: 0.46846846846846846\n",
      "COMET INFO:       DAN-support: 222\n",
      "COMET INFO:      EXP-f1-score: 0.925438596491228\n",
      "COMET INFO:     EXP-precision: 0.9701149425287356\n",
      "COMET INFO:        EXP-recall: 0.8846960167714885\n",
      "COMET INFO:       EXP-support: 477\n",
      "COMET INFO:      OIE-f1-score: 0.7361563517915309\n",
      "COMET INFO:     OIE-precision: 0.9338842975206612\n",
      "COMET INFO:        OIE-recall: 0.6075268817204301\n",
      "COMET INFO:       OIE-support: 186\n",
      "COMET INFO:      OIG-f1-score: 0.8526645768025078\n",
      "COMET INFO:     OIG-precision: 0.9444444444444444\n",
      "COMET INFO:        OIG-recall: 0.7771428571428571\n",
      "COMET INFO:       OIG-support: 175\n",
      "COMET INFO:      SEG-f1-score: 0.7353951890034364\n",
      "COMET INFO:     SEG-precision: 0.963963963963964\n",
      "COMET INFO:        SEG-recall: 0.5944444444444444\n",
      "COMET INFO:       SEG-support: 180\n",
      "COMET INFO:          accuracy: 0.9548781190572235\n",
      "COMET INFO: ----------------------------\n",
      "COMET INFO: Uploading stats to Comet before program termination (may take several seconds)\n"
     ]
    }
   ],
   "source": [
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
